# overparam

Overparameterization of linear and convolution layers in Pytorch.
Overparameterization replaces a linear layer with several larger layers during training, and collapses them at inference time.
Although the two are mathematically equivalent, the networks with additional parameters tends to exhibit better training behaviour and final accuracy. 

It implements the ideas presented in [ExpandNets: Linear Over-parameterization to Train Compact Convolutional Networks](https://arxiv.org/abs/1811.10495).

This code is based on the [Pytorch code for weight normalization](https://pytorch.org/docs/stable/generated/torch.nn.utils.weight_norm.html?highlight=weight%20norm#torch.nn.utils.weight_norm).
It adds the overparameterization as a forward hook, so that the performance penalty during training is minimal.

## Usage

```python
import torch.nn as nn
from overparam import overparam, remove_overparam

m = nn.Conv2d(10, 20, kernel_size=3)
overparam(m, expansion=4)

remove_overparam(m)
```

